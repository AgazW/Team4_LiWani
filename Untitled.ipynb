{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The notebook will try to implement various Machine Learning models to predict variables such as age, ancestry, disease and family based from OTU information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Row.names  188753  181342  3589405  4467993  355102  4465907  4465905  \\\n",
      "1     77.TS1       0       0        0        0       0        0        0   \n",
      "2   77.TS1.2       0       0        0        0       0        0        0   \n",
      "3    77.TS10       0       0        0        0       0        0        0   \n",
      "4  77.TS10.2       0       0        0        0       0        1        0   \n",
      "5   77.TS100       1       0        0        0       1        0        0   \n",
      "\n",
      "   198956  177313  ...  288224  4439443  259955  age  ancestry  family  \\\n",
      "1       0      13  ...       0        0       0   28        EA       1   \n",
      "2       0       2  ...       0        0       0   28        EA       1   \n",
      "3       0       0  ...       0        0       0   26        AA       4   \n",
      "4       1       0  ...       0        0       0   26        AA       4   \n",
      "5       1       0  ...       2        0       0   23        AA      29   \n",
      "\n",
      "   obesitycat     sex  twin_mother  zygosity  \n",
      "1        Lean  female         Twin        MZ  \n",
      "2        Lean  female         Twin        MZ  \n",
      "3       Obese  female         Twin        MZ  \n",
      "4       Obese  female         Twin        MZ  \n",
      "5       Obese  female         Twin        MZ  \n",
      "\n",
      "[5 rows x 5470 columns]\n"
     ]
    }
   ],
   "source": [
    "otu_df_pheno = pd.read_csv(\"/Users/hussainwani/Codeathon/Microbiome Data/Obese_Lean_Microbiome_Data_sub.csv\", index_col = 0)\n",
    "print(otu_df_pheno.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           188753  181342  3589405  4467993  355102  4465907  4465905  198956  \\\n",
      "77.TS127        1       0        0        0       0        0        0       0   \n",
      "77.TS31.2       2       0        0        0       2        6        0       1   \n",
      "77.TS49         1       0        0        0      66        3        0       3   \n",
      "77.TS31         1       0        1        0       6       51        0      14   \n",
      "77.TS30         4       0        0        0       9        7        0       6   \n",
      "\n",
      "           177313  177310  ...  310490  4366889  352243  354850  849704  \\\n",
      "77.TS127        0       0  ...       0        0       0       0       0   \n",
      "77.TS31.2       4       2  ...       0        0       0       0       0   \n",
      "77.TS49         0       0  ...       0        0       0       0       0   \n",
      "77.TS31        47      21  ...       0        0       0       0       0   \n",
      "77.TS30         0       0  ...       0        0       1       0       0   \n",
      "\n",
      "           4475642  300647  288224  4439443  259955  \n",
      "77.TS127         0       0       0        0       0  \n",
      "77.TS31.2        0       0       0        0       5  \n",
      "77.TS49          0       0       0        0       0  \n",
      "77.TS31          0       0       0        0       2  \n",
      "77.TS30          0       1       0        0       2  \n",
      "\n",
      "[5 rows x 5462 columns]\n",
      "Otu dimension : (281, 5462)\n"
     ]
    }
   ],
   "source": [
    "otu_df = pd.read_csv(\"/Users/hussainwani/Codeathon/Microbiome Data/OTUdata.csv\", index_col = 0)\n",
    "print(otu_df.head())\n",
    "print(\"Otu dimension :\", otu_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             age age_unit  altitude ancestry anonymized_name  body_habitat  \\\n",
      "sample_name                                                                  \n",
      "77.TS1        28    years         0       EA        sample 1  UBERON:feces   \n",
      "77.TS1.2      28    years         0       EA        sample 2  UBERON:feces   \n",
      "77.TS10       26    years         0       AA        sample 3  UBERON:feces   \n",
      "77.TS10.2     26    years         0       AA        sample 4  UBERON:feces   \n",
      "77.TS100      23    years         0       AA        sample 5  UBERON:feces   \n",
      "\n",
      "             body_product     body_site collection_timestamp collectionmet  \\\n",
      "sample_name                                                                  \n",
      "77.TS1       UBERON:feces  UBERON:feces          3/1/07 0:00      Contents   \n",
      "77.TS1.2     UBERON:feces  UBERON:feces          3/1/07 0:00      Contents   \n",
      "77.TS10      UBERON:feces  UBERON:feces          3/1/07 0:00      Contents   \n",
      "77.TS10.2    UBERON:feces  UBERON:feces          3/1/07 0:00      Contents   \n",
      "77.TS100     UBERON:feces  UBERON:feces          3/1/07 0:00      Contents   \n",
      "\n",
      "             ... physical_specimen_remaining  public qiita_study_id  \\\n",
      "sample_name  ...                                                      \n",
      "77.TS1       ...                       False    True             77   \n",
      "77.TS1.2     ...                       False    True             77   \n",
      "77.TS10      ...                       False    True             77   \n",
      "77.TS10.2    ...                       False    True             77   \n",
      "77.TS100     ...                       False    True             77   \n",
      "\n",
      "            sample_type  sampledate scientific_name     sex  \\\n",
      "sample_name                                                   \n",
      "77.TS1            stool  TimePoint1    Homo sapiens  female   \n",
      "77.TS1.2          stool  TimePoint2    Homo sapiens  female   \n",
      "77.TS10           stool  TimePoint1    Homo sapiens  female   \n",
      "77.TS10.2         stool  TimePoint2    Homo sapiens  female   \n",
      "77.TS100          stool  TimePoint1    Homo sapiens  female   \n",
      "\n",
      "                                                     title twin_mother  \\\n",
      "sample_name                                                              \n",
      "77.TS1       A core gut microbiome in obese and lean twins        Twin   \n",
      "77.TS1.2     A core gut microbiome in obese and lean twins        Twin   \n",
      "77.TS10      A core gut microbiome in obese and lean twins        Twin   \n",
      "77.TS10.2    A core gut microbiome in obese and lean twins        Twin   \n",
      "77.TS100     A core gut microbiome in obese and lean twins        Twin   \n",
      "\n",
      "             zygosity  \n",
      "sample_name            \n",
      "77.TS1             MZ  \n",
      "77.TS1.2           MZ  \n",
      "77.TS10            MZ  \n",
      "77.TS10.2          MZ  \n",
      "77.TS100           MZ  \n",
      "\n",
      "[5 rows x 40 columns]\n",
      "Metadata dimension : (281, 40)\n"
     ]
    }
   ],
   "source": [
    "meta_df = pd.read_csv(\"/Users/hussainwani/Codeathon/Microbiome Data/MetaData.csv\", index_col =0)\n",
    "print(meta_df.head())\n",
    "print(\"Metadata dimension :\", meta_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if metadata and OTUs are in order\n",
    "(otu_df.index == meta_df.index).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>188753</th>\n",
       "      <th>181342</th>\n",
       "      <th>3589405</th>\n",
       "      <th>4467993</th>\n",
       "      <th>355102</th>\n",
       "      <th>4465907</th>\n",
       "      <th>4465905</th>\n",
       "      <th>198956</th>\n",
       "      <th>177313</th>\n",
       "      <th>177310</th>\n",
       "      <th>...</th>\n",
       "      <th>310490</th>\n",
       "      <th>4366889</th>\n",
       "      <th>352243</th>\n",
       "      <th>354850</th>\n",
       "      <th>849704</th>\n",
       "      <th>4475642</th>\n",
       "      <th>300647</th>\n",
       "      <th>288224</th>\n",
       "      <th>4439443</th>\n",
       "      <th>259955</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77.TS1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77.TS1.2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77.TS10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77.TS10.2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77.TS100</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5462 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           188753  181342  3589405  4467993  355102  4465907  4465905  198956  \\\n",
       "77.TS1          0       0        0        0       0        0        0       0   \n",
       "77.TS1.2        0       0        0        0       0        0        0       0   \n",
       "77.TS10         0       0        0        0       0        0        0       0   \n",
       "77.TS10.2       0       0        0        0       0        1        0       1   \n",
       "77.TS100        1       0        0        0       1        0        0       1   \n",
       "\n",
       "           177313  177310  ...  310490  4366889  352243  354850  849704  \\\n",
       "77.TS1         13       0  ...       0        0       0       0       0   \n",
       "77.TS1.2        2       0  ...       0        0       0       0       0   \n",
       "77.TS10         0       0  ...       0        0       0       0       0   \n",
       "77.TS10.2       0       0  ...       0        0       0       0       0   \n",
       "77.TS100        0       0  ...       0        0       0       0       0   \n",
       "\n",
       "           4475642  300647  288224  4439443  259955  \n",
       "77.TS1           0       0       0        0       0  \n",
       "77.TS1.2         0       0       0        0       0  \n",
       "77.TS10          0       0       0        0       0  \n",
       "77.TS10.2        0       0       0        0       0  \n",
       "77.TS100         0       0       2        0       0  \n",
       "\n",
       "[5 rows x 5462 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort OTU data based on rownames\n",
    "sorted_otu_df = otu_df.sort_index()\n",
    "sorted_otu_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             age age_unit  altitude ancestry anonymized_name  body_habitat  \\\n",
      "sample_name                                                                  \n",
      "77.TS1        28    years         0       EA        sample 1  UBERON:feces   \n",
      "77.TS1.2      28    years         0       EA        sample 2  UBERON:feces   \n",
      "77.TS10       26    years         0       AA        sample 3  UBERON:feces   \n",
      "77.TS10.2     26    years         0       AA        sample 4  UBERON:feces   \n",
      "77.TS100      23    years         0       AA        sample 5  UBERON:feces   \n",
      "\n",
      "             body_product     body_site collection_timestamp collectionmet  \\\n",
      "sample_name                                                                  \n",
      "77.TS1       UBERON:feces  UBERON:feces          3/1/07 0:00      Contents   \n",
      "77.TS1.2     UBERON:feces  UBERON:feces          3/1/07 0:00      Contents   \n",
      "77.TS10      UBERON:feces  UBERON:feces          3/1/07 0:00      Contents   \n",
      "77.TS10.2    UBERON:feces  UBERON:feces          3/1/07 0:00      Contents   \n",
      "77.TS100     UBERON:feces  UBERON:feces          3/1/07 0:00      Contents   \n",
      "\n",
      "             ... public  qiita_study_id sample_type  sampledate  \\\n",
      "sample_name  ...                                                  \n",
      "77.TS1       ...   True              77       stool  TimePoint1   \n",
      "77.TS1.2     ...   True              77       stool  TimePoint2   \n",
      "77.TS10      ...   True              77       stool  TimePoint1   \n",
      "77.TS10.2    ...   True              77       stool  TimePoint2   \n",
      "77.TS100     ...   True              77       stool  TimePoint1   \n",
      "\n",
      "             scientific_name     sex  \\\n",
      "sample_name                            \n",
      "77.TS1          Homo sapiens  female   \n",
      "77.TS1.2        Homo sapiens  female   \n",
      "77.TS10         Homo sapiens  female   \n",
      "77.TS10.2       Homo sapiens  female   \n",
      "77.TS100        Homo sapiens  female   \n",
      "\n",
      "                                                     title twin_mother  \\\n",
      "sample_name                                                              \n",
      "77.TS1       A core gut microbiome in obese and lean twins        Twin   \n",
      "77.TS1.2     A core gut microbiome in obese and lean twins        Twin   \n",
      "77.TS10      A core gut microbiome in obese and lean twins        Twin   \n",
      "77.TS10.2    A core gut microbiome in obese and lean twins        Twin   \n",
      "77.TS100     A core gut microbiome in obese and lean twins        Twin   \n",
      "\n",
      "            zygosity  code  \n",
      "sample_name                 \n",
      "77.TS1            MZ     0  \n",
      "77.TS1.2          MZ     0  \n",
      "77.TS10           MZ     1  \n",
      "77.TS10.2         MZ     1  \n",
      "77.TS100          MZ     1  \n",
      "\n",
      "[5 rows x 41 columns]\n"
     ]
    }
   ],
   "source": [
    "sorted_meta_df = meta_df.sort_index()\n",
    "sorted_meta_df.head()\n",
    "sorted_meta_df.obesitycat = pd.Categorical(sorted_meta_df.obesitycat)\n",
    "sorted_meta_df['code'] = sorted_meta_df.obesitycat.cat.codes\n",
    "print(sorted_meta_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we will again check if the metadata and OTU data is sorted\n",
    "(sorted_meta_df.index == sorted_otu_df.index).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['181342', '3589405', '4467993', '355102'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "feature_lis = sorted_otu_df.columns\n",
    "print(feature_lis[1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1]\n",
      "sample_name\n",
      "77.TS1.2      Lean\n",
      "77.TS10      Obese\n",
      "77.TS10.2    Obese\n",
      "77.TS100     Obese\n",
      "Name: obesitycat, dtype: category\n",
      "Categories (3, object): [Lean, Obese, Overweight]\n"
     ]
    }
   ],
   "source": [
    "# Divide the data into training and testing\n",
    "feat_labels_cat = sorted_meta_df[\"obesitycat\"]\n",
    "feat_labels = np.array(sorted_meta_df[\"code\"])\n",
    "# feat_labels.reset_index(drop = True, inplace = True)\n",
    "print(feat_labels[1:5])\n",
    "print(feat_labels_cat[1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code splits the data sets with another single line:\n",
    "# split into traning and testing\n",
    "# Using Skicit-learn to split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(sorted_otu_df, feat_labels, \n",
    "                                                                          test_size = 0.25, random_state = 42)\n",
    "\n",
    "# # binarize class labels to plot ROC\n",
    "# train_labels_bnry = label_binarize(train_labels, classes=['Lean', 'Obese', 'Overweight'])\n",
    "# test_labels_bnry = label_binarize(test_labels, classes=['Lean', 'Obese', 'Overweight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Labels : [1 1 1 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 2 2 1 0 2 1 0 1 1 1 0 1 0 0 1 0 1 0 1\n",
      " 1 2 1 1 1 1 1 0 1 1 1 2 1 1 1 1 0 1 1 1 1 0 1 0 1 2 1 1 2 1 1 1 1 1 0 1 2\n",
      " 1 2 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 2 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 2 1 0 1 1 1 1 1 0 1 1 0 1 1\n",
      " 0 0 1 1 1 1 1 2 1 1 1 1 1 1 0 2 1 1 1 1 1 0 1 1 1 1 2 1 1 1 1 1 1 0 1 0 1\n",
      " 0 1 1 1 0 1 2 0 0 1 1 0 1 0 2 1 2 1 2 1 0 1 1 1 0]\n",
      "Test Labels : [1 1 1 1 0 1 0 1 1 1 1 2 1 1 2 1 1 1 1 1 1 1 2 1 0 1 1 1 1 1 0 0 1 1 0 1 0\n",
      " 0 1 2 1 0 1 1 0 1 2 1 1 0 1 1 1 1 2 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Printing some information\n",
    "print(\"Train Labels :\", train_labels)\n",
    "print(\"Test Labels :\", test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Labels Binary : [1 1 1 1]\n",
      "Test Labels Binary : [1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Labels after conversion into binary\n",
    "print(\"Train Labels Binary :\", train_labels[1:5])\n",
    "print(\"Test Labels Binary :\", test_labels[1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data dim : (210, 5462)\n",
      "Test data dim : (71, 5462)\n",
      "Train labels data dim : (210, 3)\n",
      "Test labels data dim : (71, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data dim :\", train_features.shape)\n",
    "print(\"Test data dim :\", test_features.shape)\n",
    "print(\"Train labels data dim :\", train_labels_bnry.shape)\n",
    "print(\"Test labels data dim :\", test_labels_bnry.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "                       n_jobs=-1, oob_score=False, random_state=42, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_clf = RandomForestClassifier(n_estimators = 1000, random_state=42, n_jobs=-1)\n",
    "rnd_clf.fit(train_features, train_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1]\n",
      "Test labesl:  [1 1 1 1 0 1 0 1 1 1 1 2 1 1 2 1 1 1 1 1 1 1 2 1 0 1 1 1 1 1 0 0 1 1 0 1 0\n",
      " 0 1 2 1 0 1 1 0 1 2 1 1 0 1 1 1 1 2 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1]\n",
      "Accuracy: 0.7323943661971831\n"
     ]
    }
   ],
   "source": [
    "#Prediction\n",
    "n_classes = 3\n",
    "from sklearn import metrics\n",
    "y_pred_rf = rnd_clf.predict(test_features)\n",
    "print(y_pred_rf)\n",
    "print(\"Test labesl: \", test_labels)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_labels, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hussainwani/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/hussainwani/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/hussainwani/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Ensembl classifier\n",
    "log_clf = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\", C=10) \n",
    "rnd_clf = RandomForestClassifier() \n",
    "svm_clf = SVC()\n",
    "\n",
    "# Create a voting classifier\n",
    "voting_clf = VotingClassifier(\n",
    "estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)], voting='hard'\n",
    ")\n",
    "\n",
    "\n",
    "prediction = voting_clf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hussainwani/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multiclass-multioutput and multiclass targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-264-c3b492afd94f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 81\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multiclass-multioutput and multiclass targets"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf): \n",
    "    clf.fit(train_features, train_labels)\n",
    "    y_pred = clf.predict(test_features)\n",
    "    print(clf.__class__.__name__, accuracy_score(test_features, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hussainwani/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "log_reg = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\", C=10)\n",
    "log_reg.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now predict\n",
    "y_proba = log_reg.predict_proba(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.61657006e-22, 1.00000000e+00, 2.33341705e-13],\n",
       "       [6.80590727e-20, 1.00000000e+00, 1.51904046e-10],\n",
       "       [1.88760671e-08, 9.99999979e-01, 2.02782873e-09],\n",
       "       [2.02104726e-04, 9.99791271e-01, 6.62434741e-06],\n",
       "       [9.99991198e-01, 3.02243889e-06, 5.77920020e-06],\n",
       "       [2.26382089e-01, 7.73613089e-01, 4.82207446e-06],\n",
       "       [2.24198581e-01, 7.72489914e-01, 3.31150484e-03],\n",
       "       [1.12800968e-11, 1.44951510e-10, 1.00000000e+00],\n",
       "       [1.80015407e-04, 9.99819984e-01, 5.14883370e-10],\n",
       "       [2.26512302e-09, 9.69300257e-01, 3.06997409e-02],\n",
       "       [7.90983434e-07, 9.99999209e-01, 1.95762424e-11],\n",
       "       [2.15213349e-39, 1.00000000e+00, 2.91749770e-21],\n",
       "       [7.78279481e-10, 9.99999995e-01, 3.83941549e-09],\n",
       "       [9.67599509e-01, 2.66517678e-02, 5.74872290e-03],\n",
       "       [1.70169131e-08, 3.21738035e-01, 6.78261948e-01],\n",
       "       [1.34078631e-07, 9.99992851e-01, 7.01508330e-06],\n",
       "       [7.44921254e-13, 9.99813627e-01, 1.86373170e-04],\n",
       "       [6.78122525e-13, 1.00000000e+00, 6.67446152e-20],\n",
       "       [2.15021650e-02, 9.78373340e-01, 1.24495366e-04],\n",
       "       [6.20411328e-12, 1.00000000e+00, 2.60480737e-12],\n",
       "       [5.70118525e-02, 1.51331366e-03, 9.41474834e-01],\n",
       "       [1.75198424e-25, 1.00000000e+00, 2.54511445e-15],\n",
       "       [1.30489899e-01, 8.69023884e-01, 4.86216931e-04],\n",
       "       [2.46875507e-13, 9.99999998e-01, 1.92555221e-09],\n",
       "       [2.70991901e-02, 9.71547630e-01, 1.35318017e-03],\n",
       "       [2.37920908e-17, 1.00000000e+00, 7.39693494e-16],\n",
       "       [2.79563112e-03, 9.96229404e-01, 9.74964825e-04],\n",
       "       [1.63116162e-13, 9.99999998e-01, 2.18950490e-09],\n",
       "       [9.00839991e-07, 9.99999099e-01, 7.20058639e-20],\n",
       "       [7.44100689e-15, 1.00000000e+00, 2.14835934e-11],\n",
       "       [8.55525913e-01, 1.44474086e-01, 3.47792061e-10],\n",
       "       [7.07958694e-03, 9.92920394e-01, 1.90862128e-08],\n",
       "       [2.72708788e-17, 1.00000000e+00, 1.48402813e-19],\n",
       "       [8.66001936e-01, 1.01395580e-01, 3.26024837e-02],\n",
       "       [7.17139162e-02, 8.91206046e-01, 3.70800376e-02],\n",
       "       [5.35409485e-09, 9.99999995e-01, 3.09082921e-11],\n",
       "       [9.99999990e-01, 2.98950839e-09, 7.30491063e-09],\n",
       "       [9.48415862e-01, 5.15841348e-02, 3.34416499e-09],\n",
       "       [1.39940602e-07, 9.99999850e-01, 9.60417437e-09],\n",
       "       [6.67329361e-01, 4.70126801e-02, 2.85657959e-01],\n",
       "       [1.01121174e-15, 1.00000000e+00, 2.58833764e-18],\n",
       "       [3.09341605e-06, 9.99947283e-01, 4.96233684e-05],\n",
       "       [2.09897904e-10, 9.99999999e-01, 4.33032509e-10],\n",
       "       [7.50501333e-03, 9.92492461e-01, 2.52569576e-06],\n",
       "       [2.69904308e-01, 6.29627113e-04, 7.29466065e-01],\n",
       "       [7.08969024e-05, 9.99912866e-01, 1.62371618e-05],\n",
       "       [2.23130208e-01, 7.76869792e-01, 1.79988696e-13],\n",
       "       [4.49123149e-04, 9.98853182e-01, 6.97694556e-04],\n",
       "       [3.66223807e-02, 6.54845279e-02, 8.97893091e-01],\n",
       "       [6.75780857e-01, 3.24219143e-01, 2.43358985e-21],\n",
       "       [1.83282688e-03, 9.98167173e-01, 2.00591593e-10],\n",
       "       [3.08820657e-12, 9.99999984e-01, 1.55142171e-08],\n",
       "       [2.25783415e-07, 9.99989889e-01, 9.88493291e-06],\n",
       "       [9.99999999e-01, 1.26225541e-10, 4.84495869e-10],\n",
       "       [2.47010482e-45, 1.00000000e+00, 1.06631281e-20],\n",
       "       [2.17447280e-05, 9.99964564e-01, 1.36914938e-05],\n",
       "       [4.22511975e-05, 9.99942368e-01, 1.53806735e-05],\n",
       "       [2.28245606e-11, 1.00000000e+00, 5.07894776e-11],\n",
       "       [6.56844957e-25, 1.00000000e+00, 3.77342090e-28],\n",
       "       [4.56637728e-06, 9.99992883e-01, 2.55111495e-06],\n",
       "       [3.29425196e-01, 3.46937329e-01, 3.23637474e-01],\n",
       "       [5.58282314e-04, 9.83761144e-01, 1.56805732e-02],\n",
       "       [8.82962576e-01, 9.23458446e-02, 2.46915795e-02],\n",
       "       [1.00000000e+00, 1.39027899e-34, 1.69780044e-20],\n",
       "       [1.32499875e-10, 9.98261773e-01, 1.73822657e-03],\n",
       "       [3.08405589e-01, 3.81546240e-01, 3.10048172e-01],\n",
       "       [7.66508214e-10, 9.99999996e-01, 2.95740166e-09],\n",
       "       [2.94529388e-01, 3.62036907e-01, 3.43433704e-01],\n",
       "       [1.83423617e-14, 9.99999776e-01, 2.23687616e-07],\n",
       "       [2.88036723e-04, 1.88240357e-02, 9.80887928e-01],\n",
       "       [8.92646260e-07, 9.99995790e-01, 3.31746930e-06]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33          Obese\n",
       "158         Obese\n",
       "249         Obese\n",
       "260         Obese\n",
       "101          Lean\n",
       "9           Obese\n",
       "148          Lean\n",
       "204         Obese\n",
       "146         Obese\n",
       "157         Obese\n",
       "137         Obese\n",
       "92     Overweight\n",
       "225         Obese\n",
       "212         Obese\n",
       "42     Overweight\n",
       "278         Obese\n",
       "66          Obese\n",
       "90          Obese\n",
       "120         Obese\n",
       "144         Obese\n",
       "263         Obese\n",
       "193         Obese\n",
       "147    Overweight\n",
       "267         Obese\n",
       "46           Lean\n",
       "77          Obese\n",
       "68          Obese\n",
       "75          Obese\n",
       "219         Obese\n",
       "181         Obese\n",
       "          ...    \n",
       "132          Lean\n",
       "240         Obese\n",
       "119         Obese\n",
       "227          Lean\n",
       "248         Obese\n",
       "170    Overweight\n",
       "6           Obese\n",
       "242         Obese\n",
       "73           Lean\n",
       "84          Obese\n",
       "56          Obese\n",
       "25          Obese\n",
       "159         Obese\n",
       "178    Overweight\n",
       "19          Obese\n",
       "86          Obese\n",
       "139          Lean\n",
       "186         Obese\n",
       "5           Obese\n",
       "126         Obese\n",
       "237         Obese\n",
       "213         Obese\n",
       "164          Lean\n",
       "15          Obese\n",
       "10          Obese\n",
       "279         Obese\n",
       "37          Obese\n",
       "16          Obese\n",
       "176          Lean\n",
       "250         Obese\n",
       "Name: obesitycat, Length: 71, dtype: object"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import linear_model\n",
    "# from sklearn.ensemble import SGDClassifier\n",
    "ovo_clf = OneVsOneClassifier(linear_model.SGDClassifier(random_state=42)) \n",
    "ovo_clf.fit(train_features, train_labels)\n",
    "pred = ovo_clf.predict(test_features)\n",
    "len(ovo_clf.estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Obese', 'Obese', 'Obese', 'Obese', 'Lean', 'Lean', 'Obese',\n",
       "       'Overweight', 'Obese', 'Obese', 'Obese', 'Obese', 'Obese', 'Lean',\n",
       "       'Overweight', 'Obese', 'Obese', 'Obese', 'Obese', 'Lean', 'Obese',\n",
       "       'Obese', 'Obese', 'Obese', 'Obese', 'Obese', 'Obese', 'Obese',\n",
       "       'Obese', 'Obese', 'Lean', 'Obese', 'Obese', 'Lean', 'Obese',\n",
       "       'Obese', 'Lean', 'Obese', 'Obese', 'Lean', 'Obese', 'Obese',\n",
       "       'Obese', 'Obese', 'Lean', 'Obese', 'Lean', 'Lean', 'Overweight',\n",
       "       'Lean', 'Lean', 'Obese', 'Obese', 'Lean', 'Obese', 'Obese',\n",
       "       'Obese', 'Obese', 'Obese', 'Obese', 'Obese', 'Obese', 'Lean',\n",
       "       'Obese', 'Obese', 'Obese', 'Obese', 'Obese', 'Obese', 'Obese',\n",
       "       'Obese'], dtype=object)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33          Obese\n",
       "158         Obese\n",
       "249         Obese\n",
       "260         Obese\n",
       "101          Lean\n",
       "9           Obese\n",
       "148          Lean\n",
       "204         Obese\n",
       "146         Obese\n",
       "157         Obese\n",
       "137         Obese\n",
       "92     Overweight\n",
       "225         Obese\n",
       "212         Obese\n",
       "42     Overweight\n",
       "278         Obese\n",
       "66          Obese\n",
       "90          Obese\n",
       "120         Obese\n",
       "144         Obese\n",
       "263         Obese\n",
       "193         Obese\n",
       "147    Overweight\n",
       "267         Obese\n",
       "46           Lean\n",
       "77          Obese\n",
       "68          Obese\n",
       "75          Obese\n",
       "219         Obese\n",
       "181         Obese\n",
       "          ...    \n",
       "132          Lean\n",
       "240         Obese\n",
       "119         Obese\n",
       "227          Lean\n",
       "248         Obese\n",
       "170    Overweight\n",
       "6           Obese\n",
       "242         Obese\n",
       "73           Lean\n",
       "84          Obese\n",
       "56          Obese\n",
       "25          Obese\n",
       "159         Obese\n",
       "178    Overweight\n",
       "19          Obese\n",
       "86          Obese\n",
       "139          Lean\n",
       "186         Obese\n",
       "5           Obese\n",
       "126         Obese\n",
       "237         Obese\n",
       "213         Obese\n",
       "164          Lean\n",
       "15          Obese\n",
       "10          Obese\n",
       "279         Obese\n",
       "37          Obese\n",
       "16          Obese\n",
       "176          Lean\n",
       "250         Obese\n",
       "Name: obesitycat, Length: 71, dtype: object"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.480083857442348\n",
      "0.44257703081232497\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "preci = precision_score(test_labels, pred, average='macro') \n",
    "recal = recall_score(test_labels, pred, average='macro')\n",
    "print(preci)\n",
    "print(recal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
